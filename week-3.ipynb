{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gausian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Gaussian distributions can be standardized to the reference Gaussian distribution, which is called the standard Gaussian distribution. Standardization in general is accomplished by subtracting the center of the distribution from a given element in the distribution and dividing the result by the standard deviation of the distribution.\n",
    "\n",
    "Example:-\n",
    "Suppose the underlying population of elements is N(4,16) and one element from this population is selected. We want to find the probability that the selected element has a value less than 3.0 or greater than 6.1. In solving this problem, the relevant distribution is specified: x is N(4,16). The probability of observing x<3.0 in the distribution of x is equivalent to the probability of observing z<(3.0−4)/4=−0.25 in the standard Gaussian distribution. Going to Table 1-2, z=0.25 is approximately the 60th percentile of the standard Gaussian distribution and by symmetry z=−0.25 is approximately the 40th percentile. Thus, the probability of observing a z value less than or equal to −0.25 is approximately 0.40. The probability of observing x>6.1 is equivalent to the probability of observing z>(6.1−4)/(4)=+0.525. Table 1-2 gives the probability of observing a z<0.525 as approximately 0.70, so the probability of observing a z<0.525 approximately equals 1−0.70 or 0.30. The desired probability of observing a sample observation less than 3.0 or greater than 6.1 is the sum of 0.40 and 0.30, which is approximately 0.7 or 7 chances in 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistics regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable.\n",
    "2. Logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\n",
    "3. Logistic regression can directly predict probabilities (values that are restricted to the (0,1) interval); furthermore, those probabilities are well-calibrated when compared to the probabilities predicted by some other classifiers, such as Naive Bayes.\n",
    "4. Logistic regression preserves the marginal probabilities of the training data. The coefficients of the model also provide some hint of the relative importance of each input variable.\n",
    "5. Logistic regression is generally used where the dependent variable is Binary or Dichotomous. That means the dependent variable can take only two possible values such as “Yes or No”, “Default or No Default”, “Living or Dead”, “Responder or Non Responder”, “Yes or No” etc. Independent factors or variables can be categorical or numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning.\n",
    "2. It is a Supervised Machine Learning where the data is continuously split according to a certain parameter.\n",
    "3. Decision Tree consists of :\n",
    "    a. Nodes : Test for the value of a certain attribute.\n",
    "    b. Edges/ Branch : Correspond to the outcome of a test and connect to the next node or leaf.\n",
    "    c. Leaf nodes : Terminal nodes that predict the outcome (represent class labels or class distribution).\n",
    "4. Decision trees are built using a heuristic called recursive partitioning. This approach is also commonly known as divide and conquer because it splits the data into subsets, which are then split repeatedly into even smaller subsets, and so on and so forth until the process stops when the algorithm determines the data within the subsets are sufficiently homogenous, or another stopping criterion has been met.\n",
    "5. The following steps are followed:\n",
    "    a. Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG) (reduction in uncertainty towards the final decision).\n",
    "    b.In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each leaf node all belong to the same class.\n",
    "    c. In practice, we may set a limit on the depth of the tree to prevent overfitting. We compromise on purity here somewhat as the final leaves may still have some impurity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is an ensemble tree-based learning algorithm. The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object.\n",
    "2. It is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.\n",
    "3. It can handle thousands of input variables without variable deletion.\n",
    "4. It gives estimates of what variables that are important in the classification.\n",
    "5. It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "6. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
